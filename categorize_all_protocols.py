import requests
import base64
import json
import os
import re
import socket
import time
import concurrent.futures
from collections import defaultdict
from urllib.parse import urlparse, parse_qs, unquote
from datetime import datetime, timezone, timedelta

# === Ù…Ù†Ø§Ø¨Ø¹ Ú©Ø§Ù†ÙÛŒÚ¯ ===
SOURCES = {
    "barry-far": "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/All_Configs_Sub.txt",
    "mahdibland": "https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/sub/sub_merge.txt",
    "Epodonios": "https://raw.githubusercontent.com/Epodonios/v2ray-configs/main/All_Configs_Sub.txt",
    "soroushmirzaei": "https://raw.githubusercontent.com/soroushmirzaei/telegram-configs-collector/main/splitted/mixed"
}

# === Ø¢Ø¯Ø±Ø³ Ù…Ø®Ø§Ø²Ù† Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù„ÛŒÙ†Ú©â€ŒØ¯Ù‡ÛŒ ===
SOURCE_REPOS = {
    "barry-far": "https://github.com/barry-far/V2ray-Config",
    "mahdibland": "https://github.com/mahdibland/V2RayAggregator",
    "Epodonios": "https://github.com/Epodonios/v2ray-configs",
    "soroushmirzaei": "https://github.com/soroushmirzaei/telegram-configs-collector"
}

# === Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ ØªØ³Øª ===
FAMOUS_PORTS = {'80', '443', '8080', '8088', '2052', '2053', '2082', '2083', '2086', '2087', '2095', '2096'}
GITHUB_REPO = os.environ.get('GITHUB_REPOSITORY', 'hamed1124/port-based-v2ray-configs')
MAX_WORKERS = 100
CONNECTION_TIMEOUT = 3


def fetch_all_configs_parallel(sources_dict):
    """Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    raw_configs_list, source_stats = [], defaultdict(int)
    print("Ø´Ø±ÙˆØ¹ Ø¯Ø±ÛŒØ§ÙØª Ù…ÙˆØ§Ø²ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_source = {executor.submit(requests.get, url, timeout=120): url for url in sources_dict.values()}
        
        for future in concurrent.futures.as_completed(future_to_source):
            try:
                response = future.result()
                if response.status_code == 200 and response.text:
                    name = [k for k, v in sources_dict.items() if v == response.url][0]
                    content = response.text.strip()
                    if len(content) > 1000 and "://" not in content:
                        content = base64.b64decode(content).decode('utf-8')
                    
                    configs = content.strip().split('\n')
                    valid_configs = [line.strip() for line in configs if line.strip() and '://' in line]
                    if valid_configs:
                        raw_configs_list.extend(valid_configs)
                        source_stats[name] = len(valid_configs)
                        print(f"  âœ… {len(valid_configs)} Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø¹ØªØ¨Ø± Ø§Ø² {name} Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯.")
            except Exception as e:
                print(f"- Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø¯Ø±ÛŒØ§ÙØª ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ù…Ù†Ø¨Ø¹ Ø±Ø® Ø¯Ø§Ø¯: {e}")

    raw_total = len(raw_configs_list)
    unique_configs = list(set(raw_configs_list))
    print(f"\nØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯Ù‡: {raw_total}. ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: {len(unique_configs)}.")
    return unique_configs, source_stats, raw_total

def get_config_info(link):
    """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ø±Ø§ Ø§Ø² Ù„ÛŒÙ†Ú© Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    try:
        parsed_url = urlparse(link)
        protocol = parsed_url.scheme.lower()
        host = parsed_url.hostname
        port = str(parsed_url.port)
        name = unquote(parsed_url.fragment)

        return protocol, host, port, name
    except Exception:
        return None, None, None, None

def test_config_connection(config_link):
    """ÛŒÚ© Ú©Ø§Ù†ÙÛŒÚ¯ Ø±Ø§ Ø¨Ø§ ØªØ³Øª Ø§ØªØµØ§Ù„ TCP Ù¾ÛŒÙ†Ú¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
    _, host, port, _ = get_config_info(config_link)
    if not host or not port:
        return config_link, float('inf')
    
    try:
        start_time = time.time()
        with socket.create_connection((host, int(port)), timeout=CONNECTION_TIMEOUT):
            end_time = time.time()
            latency = (end_time - start_time) * 1000
            return config_link, latency
    except (socket.timeout, socket.error, OSError, ValueError):
        return config_link, float('inf')

def test_all_configs_parallel(configs):
    """ØªÙ…Ø§Ù… Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ ØªØ³Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
    print(f"\nØ´Ø±ÙˆØ¹ ØªØ³Øª {len(configs)} Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯...")
    all_results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_config = {executor.submit(test_config_connection, config): config for config in configs}
        
        for i, future in enumerate(concurrent.futures.as_completed(future_to_config), 1):
            try:
                result = future.result()
                all_results.append(result)
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú©Ø§Ù†ÙÛŒÚ¯: {e}")

            print(f"\rØªØ³Øª Ø´Ø¯Ù‡: {i}/{len(configs)}", end="")

    all_results.sort(key=lambda x: x[1])
    live_results = [res for res in all_results if res[1] != float('inf')]
    dead_configs = [res[0] for res in all_results if res[1] == float('inf')]
    
    print(f"\nâœ… ØªØ³Øª Ú©Ø§Ù…Ù„ Ø´Ø¯. {len(live_results)} Ú©Ø§Ù†ÙÛŒÚ¯ ÙØ¹Ø§Ù„ Ùˆ {len(dead_configs)} Ú©Ø§Ù†ÙÛŒÚ¯ ØºÛŒØ±ÙØ¹Ø§Ù„ Ù¾ÛŒØ¯Ø§ Ø´Ø¯.")
    return live_results, dead_configs

def get_tehran_time():
    tehran_tz = timezone(timedelta(hours=3, minutes=30))
    now_tehran = datetime.now(timezone.utc).astimezone(tehran_tz)
    return now_tehran.strftime("%Y-%m-%d %H:%M:%S Tehran Time")

def build_readme_content(stats):
    """Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù„ README Ø±Ø§ Ø¨Ø§ Ø¢Ù…Ø§Ø± Ø¬Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯."""
    print("\nBuilding README content...")
    
    detailed_stats = stats.get('detailed_stats', {})
    protocol_totals = {p: sum(len(cfgs) for cfgs in data.values()) for p, data in detailed_stats.items()}
    sorted_protocols = sorted(protocol_totals.keys(), key=lambda p: protocol_totals[p], reverse=True)
    port_totals = {port: sum(len(detailed_stats.get(p, {}).get(port, [])) for p in sorted_protocols) for port in FAMOUS_PORTS}
    sorted_ports = sorted(port_totals.keys(), key=lambda p: port_totals[p], reverse=True)
    
    stats_table_lines = []
    header = "| Protocol | " + " | ".join(sorted_ports) + " | Other Ports | Total |"
    separator = "|:---| " + " | ".join([":---:" for _ in sorted_ports]) + " |:---:|:---:|"
    stats_table_lines.extend([header, separator])
    total_other_ports = 0
    for proto in sorted_protocols:
        row = [f"| {proto.capitalize()}"]
        famous_ports_sum = 0
        for port in sorted_ports:
            count = len(detailed_stats.get(proto, {}).get(port, []))
            row.append(str(count))
            famous_ports_sum += count
        other_ports_count = protocol_totals[proto] - famous_ports_sum
        total_other_ports += other_ports_count
        row.append(str(other_ports_count))
        row.append(f"**{protocol_totals[proto]}**")
        stats_table_lines.append(" | ".join(row) + " |")
    footer = ["| **Total**", *[f"**{port_totals[port]}**" for port in sorted_ports], f"**{total_other_ports}**", f"**{sum(protocol_totals.values())}**"]
    stats_table_lines.append(" | ".join(footer) + " |")
    stats_table_string = "\n".join(stats_table_lines)
    
    protocol_links_string = "\n\n".join([f"- **{proto.capitalize()}:**\n  https://raw.githubusercontent.com/{GITHUB_REPO}/main/sub/protocols/{proto}.txt" for proto in sorted_protocols])
    port_links_string = "\n\n".join([f"- **Port {port}:**\n  https://raw.githubusercontent.com/{GITHUB_REPO}/main/sub/{port}.txt" for port in sorted_ports])

    source_stats_lines = []
    summary_lines = [
        f"**Total Fetched (Raw):** {stats['raw_total']}",
        f"**Duplicates Removed:** {stats['duplicates_removed']}",
        "---",
        f"**Unique Configs Tested:** {stats['tested_configs']}",
        f"**Working Configs Found:** {stats['live_configs']}",
    ]
    details_lines = [f"**[{name}]({SOURCE_REPOS.get(name, '#')}):** {count} configs" for name, count in sorted(stats['source_stats'].items(), key=lambda item: item[1], reverse=True)]
    source_stats_lines.extend(["| Summary | Source Details |", "|:---|:---|"])
    max_len = max(len(summary_lines), len(details_lines))
    for i in range(max_len):
        left_col = summary_lines[i] if i < len(summary_lines) else ""
        right_col = details_lines[i] if i < len(details_lines) else ""
        source_stats_lines.append(f"| {left_col} | {right_col} |")
    source_stats_string = "\n".join(source_stats_lines)
    
    test_results_links = f"""- **Top 200 (by Ping, US & NA excluded):** `https://raw.githubusercontent.com/{GITHUB_REPO}/main/test-results/top_200_ping.txt`
- **Dead Configs:** `https://raw.githubusercontent.com/{GITHUB_REPO}/main/test-results/dead_configs.txt`"""
    
    final_readme = f"""# Config Collector

[![Auto-Update Status](https://github.com/hamed1124/port-based-v2ray-configs/actions/workflows/main.yml/badge.svg)](https://github.com/hamed1124/port-based-v2ray-configs/actions/workflows/main.yml)

An automated repository that collects and categorizes free V2Ray/Clash configurations from reputable sources with advanced classification.

---

### ğŸ“Š Live Statistics

**Last Updated:** {stats['update_time']}

**Total Unique Configurations (Working):** {stats['total_configs']}

{stats_table_string}

---

### ğŸš€ Subscription Links

#### By Protocol

{protocol_links_string}

#### By Famous Ports

{port_links_string}

---

### ğŸ§ª Test Results
*Note: The Top 200 list is sorted by the lowest ping and excludes US & NA-based servers for better performance in most regions.*

{test_results_links}

---

### ğŸ“š Sources

{source_stats_string}
"""
    return final_readme


def main():
    unique_configs, source_stats, raw_total = fetch_all_configs_parallel(SOURCES)
    if not unique_configs:
        print("\nNo configs found. Exiting.")
        return

    live_results, dead_configs = test_all_configs_parallel(unique_configs)
    
    # --- *** Ù…Ù†Ø·Ù‚ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† US Ùˆ NA *** ---
    print("\nFiltering out US & NA-based servers from the top list...")
    excluded_identifiers = [' us', '.us', '-us', '_us', 'united states', 'ğŸ‡ºğŸ‡¸', ' na', '.na', '-na', '_na', 'not available']
    
    filtered_live_configs = []
    for config, latency in live_results:
        # Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙÙ‚Ø· Ø¨Ù‡ Ù†Ø§Ù… Ú©Ø§Ù†ÙÛŒÚ¯ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ…
        _, _, _, name = get_config_info(config)
        
        is_excluded = False
        if name: # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù†Ø§Ù… ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
            for identifier in excluded_identifiers:
                if identifier in name.lower():
                    is_excluded = True
                    break
        
        if not is_excluded:
            filtered_live_configs.append(config)
            
    # Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Û²Û°Û°ØªØ§ÛŒ Ø¨Ø±ØªØ± Ø§Ø² Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡
    top_200_configs = filtered_live_configs[:200]
    print(f"Generated a top list of {len(top_200_configs)} configs (US & NA excluded).")

    # Ù„ÛŒØ³Øª Ú©Ù„ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ (Ø´Ø§Ù…Ù„ Ù‡Ù…Ù‡ Ú©Ø´ÙˆØ±Ù‡Ø§) Ø¨Ø±Ø§ÛŒ Ø¨Ù‚ÛŒÙ‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    live_configs = [res[0] for res in live_results]
    
    print("\nWriting test result files...")
    os.makedirs('test-results', exist_ok=True)
    with open('test-results/dead_configs.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(dead_configs))
    with open('test-results/top_200_ping.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(top_200_configs))
    print("âœ… Test result files created successfully.")
    
    if not live_configs:
        print("\nNo working configs found after testing. Exiting.")
        stats = {
            "total_configs": 0, "raw_total": raw_total, "duplicates_removed": raw_total - len(unique_configs),
            "tested_configs": len(unique_configs), "live_configs": 0, "update_time": get_tehran_time(),
            "source_stats": source_stats, "detailed_stats": {}
        }
        final_readme_content = build_readme_content(stats)
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(final_readme_content)
        return

    print("\nCategorizing all working configurations...")
    categorized_by_protocol_and_port = defaultdict(lambda: defaultdict(list))
    for config_link in live_configs:
        protocol, _, port, _ = get_config_info(config_link)
        if protocol and port:
            categorized_by_protocol_and_port[protocol][port].append(config_link)

    print("\nWriting all subscription files based on working configs...")
    os.makedirs('sub/protocols', exist_ok=True)
    with open('sub/all.txt', 'w', encoding='utf-8') as f: f.write('\n'.join(live_configs))
    for protocol, ports_data in categorized_by_protocol_and_port.items():
        all_protocol_configs = [cfg for cfgs in ports_data.values() for cfg in cfgs]
        with open(f'sub/protocols/{protocol}.txt', 'w', encoding='utf-8') as f: f.write('\n'.join(all_protocol_configs))
    for port in FAMOUS_PORTS:
        port_configs = [cfg for p_data in categorized_by_protocol_and_port.values() for p, cfgs in p_data.items() if p == port for cfg in cfgs]
        if port_configs:
            with open(f'sub/{port}.txt', 'w', encoding='utf-8') as f: f.write('\n'.join(port_configs))
    detailed_folder = 'detailed'
    os.makedirs(detailed_folder, exist_ok=True)
    for protocol, ports_data in categorized_by_protocol_and_port.items():
        protocol_folder = os.path.join(detailed_folder, protocol)
        os.makedirs(protocol_folder, exist_ok=True)
        other_ports_folder = os.path.join(protocol_folder, 'other_ports')
        has_other_ports = False
        for port, configs in ports_data.items():
            file_path = os.path.join(protocol_folder, f'{port}.txt') if port in FAMOUS_PORTS else os.path.join(other_ports_folder, f'{port}.txt')
            if port not in FAMOUS_PORTS and not has_other_ports:
                os.makedirs(other_ports_folder, exist_ok=True)
                has_other_ports = True
            with open(file_path, 'w', encoding='utf-8') as f: f.write('\n'.join(configs))
    print("âœ… All subscription files written successfully.")

    stats = {
        "total_configs": len(live_configs),
        "raw_total": raw_total,
        "duplicates_removed": raw_total - len(unique_configs),
        "tested_configs": len(unique_configs),
        "live_configs": len(live_configs),
        "update_time": get_tehran_time(),
        "source_stats": source_stats,
        "detailed_stats": categorized_by_protocol_and_port
    }import requests
import base64
import json
import os
import re
import socket
import time
import concurrent.futures
from collections import defaultdict
from urllib.parse import urlparse, parse_qs, unquote
from datetime import datetime, timezone, timedelta

# === Ù…Ù†Ø§Ø¨Ø¹ Ú©Ø§Ù†ÙÛŒÚ¯ ===
SOURCES = {
    "barry-far": "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/All_Configs_Sub.txt",
    "mahdibland": "https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/sub/sub_merge.txt",
    "Epodonios": "https://raw.githubusercontent.com/Epodonios/v2ray-configs/main/All_Configs_Sub.txt",
    "soroushmirzaei": "https://raw.githubusercontent.com/soroushmirzaei/telegram-configs-collector/main/splitted/mixed"
}

# === Ø¢Ø¯Ø±Ø³ Ù…Ø®Ø§Ø²Ù† Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù„ÛŒÙ†Ú©â€ŒØ¯Ù‡ÛŒ ===
SOURCE_REPOS = {
    "barry-far": "https://github.com/barry-far/V2ray-Config",
    "mahdibland": "https://github.com/mahdibland/V2RayAggregator",
    "Epodonios": "https://github.com/Epodonios/v2ray-configs",
    "soroushmirzaei": "https://github.com/soroushmirzaei/telegram-configs-collector"
}

# === Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ ØªØ³Øª ===
FAMOUS_PORTS = {'80', '443', '8080', '8088', '2052', '2053', '2082', '2083', '2086', '2087', '2095', '2096'}
GITHUB_REPO = os.environ.get('GITHUB_REPOSITORY', 'hamed1124/port-based-v2ray-configs')
MAX_WORKERS = 100
CONNECTION_TIMEOUT = 3


def fetch_all_configs_parallel(sources_dict):
    """Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    raw_configs_list, source_stats = [], defaultdict(int)
    print("Ø´Ø±ÙˆØ¹ Ø¯Ø±ÛŒØ§ÙØª Ù…ÙˆØ§Ø²ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_source = {executor.submit(requests.get, url, timeout=120): url for url in sources_dict.values()}
        
        for future in concurrent.futures.as_completed(future_to_source):
            try:
                response = future.result()
                if response.status_code == 200 and response.text:
                    name = [k for k, v in sources_dict.items() if v == response.url][0]
                    content = response.text.strip()
                    if len(content) > 1000 and "://" not in content:
                        content = base64.b64decode(content).decode('utf-8')
                    
                    configs = content.strip().split('\n')
                    valid_configs = [line.strip() for line in configs if line.strip() and '://' in line]
                    if valid_configs:
                        raw_configs_list.extend(valid_configs)
                        source_stats[name] = len(valid_configs)
                        print(f"  âœ… {len(valid_configs)} Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø¹ØªØ¨Ø± Ø§Ø² {name} Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯.")
            except Exception as e:
                print(f"- Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø¯Ø±ÛŒØ§ÙØª ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ù…Ù†Ø¨Ø¹ Ø±Ø® Ø¯Ø§Ø¯: {e}")

    raw_total = len(raw_configs_list)
    unique_configs = list(set(raw_configs_list))
    print(f"\nØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯Ù‡: {raw_total}. ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: {len(unique_configs)}.")
    return unique_configs, source_stats, raw_total

def get_config_info(link):
    """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ø±Ø§ Ø§Ø² Ù„ÛŒÙ†Ú© Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    try:
        parsed_url = urlparse(link)
        protocol = parsed_url.scheme.lower()
        host = parsed_url.hostname
        port = str(parsed_url.port)
        name = unquote(parsed_url.fragment)

        return protocol, host, port, name
    except Exception:
        return None, None, None, None

def test_config_connection(config_link):
    """ÛŒÚ© Ú©Ø§Ù†ÙÛŒÚ¯ Ø±Ø§ Ø¨Ø§ ØªØ³Øª Ø§ØªØµØ§Ù„ TCP Ù¾ÛŒÙ†Ú¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
    _, host, port, _ = get_config_info(config_link)
    if not host or not port:
        return config_link, float('inf')
    
    try:
        start_time = time.time()
        with socket.create_connection((host, int(port)), timeout=CONNECTION_TIMEOUT):
            end_time = time.time()
            latency = (end_time - start_time) * 1000
            return config_link, latency
    except (socket.timeout, socket.error, OSError, ValueError):
        return config_link, float('inf')

def test_all_configs_parallel(configs):
    """ØªÙ…Ø§Ù… Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ ØªØ³Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
    print(f"\nØ´Ø±ÙˆØ¹ ØªØ³Øª {len(configs)} Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯...")
    all_results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_config = {executor.submit(test_config_connection, config): config for config in configs}
        
        for i, future in enumerate(concurrent.futures.as_completed(future_to_config), 1):
            try:
                result = future.result()
                all_results.append(result)
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú©Ø§Ù†ÙÛŒÚ¯: {e}")

            print(f"\rØªØ³Øª Ø´Ø¯Ù‡: {i}/{len(configs)}", end="")

    all_results.sort(key=lambda x: x[1])
    live_results = [res for res in all_results if res[1] != float('inf')]
    dead_configs = [res[0] for res in all_results if res[1] == float('inf')]
    
    print(f"\nâœ… ØªØ³Øª Ú©Ø§Ù…Ù„ Ø´Ø¯. {len(live_results)} Ú©Ø§Ù†ÙÛŒÚ¯ ÙØ¹Ø§Ù„ Ùˆ {len(dead_configs)} Ú©Ø§Ù†ÙÛŒÚ¯ ØºÛŒØ±ÙØ¹Ø§Ù„ Ù¾ÛŒØ¯Ø§ Ø´Ø¯.")
    return live_results, dead_configs

def get_tehran_time():
    tehran_tz = timezone(timedelta(hours=3, minutes=30))
    now_tehran = datetime.now(timezone.utc).astimezone(tehran_tz)
    return now_tehran.strftime("%Y-%m-%d %H:%M:%S Tehran Time")

def build_readme_content(stats):
    """Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù„ README Ø±Ø§ Ø¨Ø§ Ø¢Ù…Ø§Ø± Ø¬Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯."""
    print("\nBuilding README content...")
    
    detailed_stats = stats.get('detailed_stats', {})
    protocol_totals = {p: sum(len(cfgs) for cfgs in data.values()) for p, data in detailed_stats.items()}
    sorted_protocols = sorted(protocol_totals.keys(), key=lambda p: protocol_totals[p], reverse=True)
    port_totals = {port: sum(len(detailed_stats.get(p, {}).get(port, [])) for p in sorted_protocols) for port in FAMOUS_PORTS}
    sorted_ports = sorted(port_totals.keys(), key=lambda p: port_totals[p], reverse=True)
    
    stats_table_lines = []
    header = "| Protocol | " + " | ".join(sorted_ports) + " | Other Ports | Total |"
    separator = "|:---| " + " | ".join([":---:" for _ in sorted_ports]) + " |:---:|:---:|"
    stats_table_lines.extend([header, separator])
    total_other_ports = 0
    for proto in sorted_protocols:
        row = [f"| {proto.capitalize()}"]
        famous_ports_sum = 0
        for port in sorted_ports:
            count = len(detailed_stats.get(proto, {}).get(port, []))
            row.append(str(count))
            famous_ports_sum += count
        other_ports_count = protocol_totals[proto] - famous_ports_sum
        total_other_ports += other_ports_count
        row.append(str(other_ports_count))
        row.append(f"**{protocol_totals[proto]}**")
        stats_table_lines.append(" | ".join(row) + " |")
    footer = ["| **Total**", *[f"**{port_totals[port]}**" for port in sorted_ports], f"**{total_other_ports}**", f"**{sum(protocol_totals.values())}**"]
    stats_table_lines.append(" | ".join(footer) + " |")
    stats_table_string = "\n".join(stats_table_lines)
    
    protocol_links_string = "\n\n".join([f"- **{proto.capitalize()}:**\n  https://raw.githubusercontent.com/{GITHUB_REPO}/main/sub/protocols/{proto}.txt" for proto in sorted_protocols])
    port_links_string = "\n\n".join([f"- **Port {port}:**\n  https://raw.githubusercontent.com/{GITHUB_REPO}/main/sub/{port}.txt" for port in sorted_ports])

    source_stats_lines = []
    summary_lines = [
        f"**Total Fetched (Raw):** {stats['raw_total']}",
        f"**Duplicates Removed:** {stats['duplicates_removed']}",
        "---",
        f"**Unique Configs Tested:** {stats['tested_configs']}",
        f"**Working Configs Found:** {stats['live_configs']}",
    ]
    details_lines = [f"**[{name}]({SOURCE_REPOS.get(name, '#')}):** {count} configs" for name, count in sorted(stats['source_stats'].items(), key=lambda item: item[1], reverse=True)]
    source_stats_lines.extend(["| Summary | Source Details |", "|:---|:---|"])
    max_len = max(len(summary_lines), len(details_lines))
    for i in range(max_len):
        left_col = summary_lines[i] if i < len(summary_lines) else ""
        right_col = details_lines[i] if i < len(details_lines) else ""
        source_stats_lines.append(f"| {left_col} | {right_col} |")
    source_stats_string = "\n".join(source_stats_lines)
    
    test_results_links = f"""- **Top 200 (by Ping, US & NA excluded):** `https://raw.githubusercontent.com/{GITHUB_REPO}/main/test-results/top_200_ping.txt`
- **Dead Configs:** `https://raw.githubusercontent.com/{GITHUB_REPO}/main/test-results/dead_configs.txt`"""
    
    final_readme = f"""# Config Collector

[![Auto-Update Status](https://github.com/hamed1124/port-based-v2ray-configs/actions/workflows/main.yml/badge.svg)](https://github.com/hamed1124/port-based-v2ray-configs/actions/workflows/main.yml)

An automated repository that collects and categorizes free V2Ray/Clash configurations from reputable sources with advanced classification.

---

### ğŸ“Š Live Statistics

**Last Updated:** {stats['update_time']}

**Total Unique Configurations (Working):** {stats['total_configs']}

{stats_table_string}

---

### ğŸš€ Subscription Links

#### By Protocol

{protocol_links_string}

#### By Famous Ports

{port_links_string}

---

### ğŸ§ª Test Results
*Note: The Top 200 list is sorted by the lowest ping and excludes US & NA-based servers for better performance in most regions.*

{test_results_links}

---

### ğŸ“š Sources

{source_stats_string}
"""
    return final_readme


def main():
    unique_configs, source_stats, raw_total = fetch_all_configs_parallel(SOURCES)
    if not unique_configs:
        print("\nNo configs found. Exiting.")
        return

    live_results, dead_configs = test_all_configs_parallel(unique_configs)
    
    # --- *** Ù…Ù†Ø·Ù‚ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† US Ùˆ NA *** ---
    print("\nFiltering out US & NA-based servers from the top list...")
    excluded_identifiers = [' us', '.us', '-us', '_us', 'united states', 'ğŸ‡ºğŸ‡¸', ' na', '.na', '-na', '_na', 'not available']
    
    filtered_live_configs = []
    for config, latency in live_results:
        # Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙÙ‚Ø· Ø¨Ù‡ Ù†Ø§Ù… Ú©Ø§Ù†ÙÛŒÚ¯ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ…
        _, _, _, name = get_config_info(config)
        
        is_excluded = False
        if name: # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù†Ø§Ù… ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
            for identifier in excluded_identifiers:
                if identifier in name.lower():
                    is_excluded = True
                    break
        
        if not is_excluded:
            filtered_live_configs.append(config)
            
    # Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Û²Û°Û°ØªØ§ÛŒ Ø¨Ø±ØªØ± Ø§Ø² Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡
    top_200_configs = filtered_live_configs[:200]
    print(f"Generated a top list of {len(top_200_configs)} configs (US & NA excluded).")

    # Ù„ÛŒØ³Øª Ú©Ù„ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ (Ø´Ø§Ù…Ù„ Ù‡Ù…Ù‡ Ú©Ø´ÙˆØ±Ù‡Ø§) Ø¨Ø±Ø§ÛŒ Ø¨Ù‚ÛŒÙ‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    live_configs = [res[0] for res in live_results]
    
    print("\nWriting test result files...")
    os.makedirs('test-results', exist_ok=True)
    with open('test-results/dead_configs.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(dead_configs))
    with open('test-results/top_200_ping.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(top_200_configs))
    print("âœ… Test result files created successfully.")
    
    if not live_configs:
        print("\nNo working configs found after testing. Exiting.")
        stats = {
            "total_configs": 0, "raw_total": raw_total, "duplicates_removed": raw_total - len(unique_configs),
            "tested_configs": len(unique_configs), "live_configs": 0, "update_time": get_tehran_time(),
            "source_stats": source_stats, "detailed_stats": {}
        }
        final_readme_content = build_readme_content(stats)
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(final_readme_content)
        return

    print("\nCategorizing all working configurations...")
    categorized_by_protocol_and_port = defaultdict(lambda: defaultdict(list))
    for config_link in live_configs:
        protocol, _, port, _ = get_config_info(config_link)
        if protocol and port:
            categorized_by_protocol_and_port[protocol][port].append(config_link)

    print("\nWriting all subscription files based on working configs...")
    os.makedirs('sub/protocols', exist_ok=True)
    with open('sub/all.txt', 'w', encoding='utf-8') as f: f.write('\n'.join(live_configs))
    for protocol, ports_data in categorized_by_protocol_and_port.items():
        all_protocol_configs = [cfg for cfgs in ports_data.values() for cfg in cfgs]
        with open(f'sub/protocols/{protocol}.txt', 'w', encoding='utf-8') as f: f.write('\n'.join(all_protocol_configs))
    for port in FAMOUS_PORTS:
        port_configs = [cfg for p_data in categorized_by_protocol_and_port.values() for p, cfgs in p_data.items() if p == port for cfg in cfgs]
        if port_configs:
            with open(f'sub/{port}.txt', 'w', encoding='utf-8') as f: f.write('\n'.join(port_configs))
    detailed_folder = 'detailed'
    os.makedirs(detailed_folder, exist_ok=True)
    for protocol, ports_data in categorized_by_protocol_and_port.items():
        protocol_folder = os.path.join(detailed_folder, protocol)
        os.makedirs(protocol_folder, exist_ok=True)
        other_ports_folder = os.path.join(protocol_folder, 'other_ports')
        has_other_ports = False
        for port, configs in ports_data.items():
            file_path = os.path.join(protocol_folder, f'{port}.txt') if port in FAMOUS_PORTS else os.path.join(other_ports_folder, f'{port}.txt')
            if port not in FAMOUS_PORTS and not has_other_ports:
                os.makedirs(other_ports_folder, exist_ok=True)
                has_other_ports = True
            with open(file_path, 'w', encoding='utf-8') as f: f.write('\n'.join(configs))
    print("âœ… All subscription files written successfully.")

    stats = {
        "total_configs": len(live_configs),
        "raw_total": raw_total,
        "duplicates_removed": raw_total - len(unique_configs),
        "tested_configs": len(unique_configs),
        "live_configs": len(live_configs),
        "update_time": get_tehran_time(),
        "source_stats": source_stats,
        "detailed_stats": categorized_by_protocol_and_port
    }
    
    final_readme_content = build_readme_content(stats)
    
    with open('README.md', 'w', encoding='utf-8') as f:
        f.write(final_readme_content)
    print("âœ… README.md updated successfully with advanced filtering.")

    print("\nğŸ‰ Project update finished successfully.")

if __name__ == "__main__":
    main()

    
    final_readme_content = build_readme_content(stats)
    
    with open('README.md', 'w', encoding='utf-8') as f:
        f.write(final_readme_content)
    print("âœ… README.md updated successfully with advanced filtering.")

    print("\nğŸ‰ Project update finished successfully.")

if __name__ == "__main__":
    main()
